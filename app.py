
import streamlit as st
import os
import sys
import tempfile


from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- ä¿®æ­£åçš„ app.py å¯¼å…¥è¯­å¥ (æœ€ç»ˆç‰ˆæœ¬) ---

# 1. å¯¼å…¥ LangChain Chains ä¾èµ– (ä» langchain-community å°è¯•å¯¼å…¥)
# âš ï¸ æ³¨æ„ï¼šRetrievalQA åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­è¢«ç§»åˆ°äº† community åŒ…
from langchain_community.chains import RetrievalQA 

# 2. å¯¼å…¥ LangChain Core ä¾èµ–
from langchain_core.prompts import PromptTemplate # â¬…ï¸ å°† PromptTemplate ç§»åˆ° core åŒ…

# 3. å¯¼å…¥ LangChain Community ä¾èµ– (ä¿æŒä¸å˜)
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma


# --- 2. é…ç½®å’Œåˆå§‹åŒ– (Streamlit Caching) ---

# éƒ¨ç½²æ—¶ï¼Œéœ€å°† PDF æ–‡ä»¶æ”¾åœ¨ /data ç›®å½•ä¸‹
DATA_PATH = "data/NLP and Text Analysis: Introduction.pdf"
PERSIST_DIR = "./chroma_db_cache"  # æ•°æ®åº“ç¼“å­˜ç›®å½•

# âš ï¸ éƒ¨ç½²åˆ° Streamlit Cloud æ—¶ï¼ŒAPI Key å¿…é¡»é€šè¿‡ Secrets ä¼ å…¥
API_KEY = st.secrets["GOOGLE_API_KEY"]
os.environ["GOOGLE_API_KEY"] = API_KEY

# åˆå§‹åŒ–æ¨¡å‹ (ä½¿ç”¨ Streamlit çš„ Caching æœºåˆ¶ç¡®ä¿åªè¿è¡Œä¸€æ¬¡)
@st.cache_resource
def initialize_models():
    # LLM (ç”¨äºç”Ÿæˆç­”æ¡ˆ)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        convert_system_message_to_human=True,
        google_api_key=API_KEY
    )
    # Embedding Model (ç”¨äºå»ºç«‹å’ŒæŸ¥è¯¢å‘é‡)
    embedding_model = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2", 
        model_kwargs={'device': 'cpu'}, 
        encode_kwargs={'normalize_embeddings': True}
    )
    return llm, embedding_model

llm, embedding_model = initialize_models()


# --- 3. æ ¸å¿ƒï¼šå»ºç«‹æˆ–åŠ è½½ Vector Store (æ•°æ®å¤„ç†) ---

@st.cache_resource
def setup_vector_store():
    # å°è¯•åŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“
    try:
        db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding_model)
        # ç®€å•æµ‹è¯•ï¼Œç¡®ä¿æ•°æ®åº“ä¸æ˜¯ç©ºçš„
        if db._collection.count() > 0:
            st.success("âœ… å·²åŠ è½½æŒä¹…åŒ–çŸ¥è¯†åº“ã€‚")
            return db
    except:
        pass # å¦‚æœåŠ è½½å¤±è´¥æˆ–ä¸å­˜åœ¨ï¼Œåˆ™é‡æ–°å»ºç«‹

    # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œåˆ™ä»å¤´å»ºç«‹
    st.info("ğŸ”„ çŸ¥è¯†åº“ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ­£åœ¨ä» PDF æ–‡ä»¶å»ºç«‹ç´¢å¼• (è¿™åªä¼šå‘ç”Ÿä¸€æ¬¡)...")
    
    # 1. åŠ è½½æ–‡ä»¶
    loader = PyPDFLoader(DATA_PATH)
    documents = loader.load()
    
    # 2. çŸ¥è¯†åˆ‡åˆ†
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    
    # 3. å‘é‡åŒ–å’Œå­˜å‚¨
    db = Chroma.from_documents(
        documents=chunks, 
        embedding=embedding_model, 
        persist_directory=PERSIST_DIR 
    )
    st.success(f"âœ… çŸ¥è¯†åº“å»ºç«‹å®Œæˆï¼å…±ç´¢å¼• {db._collection.count()} ä¸ªçŸ¥è¯†å—ã€‚")
    return db

db = setup_vector_store()

# --- 4. Prompt å®šä¹‰å’Œ Q&A é“¾è®¾ç½® ---

TEMPLATE = """
## I. Role and Persona (System Role)
You are a **highly constrained, specialized University Teaching Assistant**. Your **SOLE** source of knowledge is the provided ã€Reference Materialã€‘. You **MUST NOT** use any external knowledge, common sense, or pre-existing training data. Your primary goal is to ensure the student's answer is verifiable within the provided context.

## II. RAG Action Rules (The Logic)
1. **Source Constraint:** Your answers **MUST** be entirely and **EXCLUSIVELY** based on the content found in the ã€Reference Materialã€‘. If the document does not contain a specific answer, you must proceed to the Refusal Rule immediately.
2. **Refusal Rule:** If the ã€Reference Materialã€‘ does not cover the exact question, you **MUST** politely decline and state: "I apologize, this specific information is not covered in the course material." Do not guess or infer.
3. **Comparison Rule:** If the student asks about a difference or comparison between two concepts (e.g., two algorithms or methods), you must present the answer using a clear, side-by-side comparison (table or bullet points).

## III. Knowledge Insertion
ã€Reference Materialã€‘:
{context}

## IV. Student Query
{question}
"""

CUSTOM_PROMPT = PromptTemplate(template=TEMPLATE, input_variables=["context", "question"])

retriever = db.as_retriever(search_kwargs={"k": 4}) 

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", 
    retriever=retriever,
    chain_type_kwargs={"prompt": CUSTOM_PROMPT}, 
    return_source_documents=False 
)


# --- 5. Streamlit ç•Œé¢é€»è¾‘ ---
# --- 5. Streamlit ç•Œé¢é€»è¾‘ (å®Œæ•´äº¤äº’ç‰ˆæœ¬) ---

st.title("ğŸ“š NLP AI Agent (RAG)")
st.caption("âœ… çŸ¥è¯†åº“å·²ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ã€‚")

# 1. åˆå§‹åŒ–èŠå¤©è®°å½• (Chat History)
# ç¡®ä¿èŠå¤©è®°å½•åœ¨ä¼šè¯çŠ¶æ€ä¸­æ˜¯æŒä¹…åŒ–çš„
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ NLP è¯¾ç¨‹åŠ©æ•™ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå…³äºè¯¾ç¨‹çš„é—®é¢˜å—ï¼Ÿ"}
    ]

# 2. æ˜¾ç¤ºæ‰€æœ‰å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    # ä½¿ç”¨ st.chat_message è‡ªåŠ¨ç¾åŒ–æ¶ˆæ¯æ°”æ³¡
    st.chat_message(msg["role"]).write(msg["content"])

# 3. æ¥æ”¶ç”¨æˆ·è¾“å…¥ (Input)
# st.chat_input ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªè¾“å…¥æ¡†å¹¶å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ£€æŸ¥ API Key æ˜¯å¦è®¾ç½®
    if not API_KEY:
        st.error("éƒ¨ç½²é”™è¯¯ï¼šè¯·åœ¨ Streamlit Secrets ä¸­è®¾ç½® GOOGLE_API_KEYã€‚")
        # ç«‹å³åœæ­¢æ‰§è¡Œåç»­é€»è¾‘
        sys.exit() 
    
    # a. å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½•å¹¶æ˜¾ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    # b. è°ƒç”¨ Agent è¿›è¡Œæ£€ç´¢å’Œç”Ÿæˆ (Output)
    with st.spinner("ğŸ§  åŠ©æ•™æ­£åœ¨æŸ¥é˜…çŸ¥è¯†åº“å¹¶æ€è€ƒ..."):
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œä½ éœ€è¦è°ƒç”¨ä¹‹å‰åœ¨ app.py ä¸­å®šä¹‰çš„ qa_chain.invoke
        # ç¡®ä¿è¿™ä¸ªå‡½æ•°åœ¨å½“å‰ä½œç”¨åŸŸå¯ç”¨
        try:
            # è°ƒç”¨ qa_chain (æˆ‘ä»¬å‡è®¾å®ƒæ˜¯å¯ç”¨çš„)
            result = qa_chain.invoke({"query": prompt})
            response = result['result']
        except Exception as e:
            response = f"âŒ å†…éƒ¨é”™è¯¯ï¼šAgent æ— æ³•å¤„ç†è¯·æ±‚ã€‚é”™è¯¯è¯¦æƒ…: {e}"

    # c. å°† Agent å›ç­”æ·»åŠ åˆ°å†å²è®°å½•å¹¶æ˜¾ç¤º
    st.chat_message("assistant").write(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
